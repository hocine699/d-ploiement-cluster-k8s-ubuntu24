# ============================================================
# ðŸš€ Pipeline GitLab CI/CD : DÃ©ploiement complet dâ€™un cluster K8s
# Auteur : Hocine 
# Objectif : Automatiser le dÃ©ploiement K8s (Master + 2 Workers)
# ============================================================

stages:
  - Nettoyage
  - Validation
  - PrÃ©paration
  - DÃ©ploiement-cluster
  - VÃ©rification
  - DÃ©ploiement-app
  - Maintenance

# ============================================================
# ðŸŒ Variables globales
# ============================================================

variables:
  ANSIBLE_FORCE_COLOR: "True"
  ANSIBLE_STDOUT_CALLBACK: "yaml"
  ANSIBLE_HOST_KEY_CHECKING: "False"

# ============================================================
# âš™ï¸ Template de base pour tous les jobs Ansible
# ============================================================

.ansible_job: &ansible_job
  image: mon-runner-devops:latest
  tags:
    - synology
  before_script:
    # ðŸ”¹ Copier le repo dans un dossier sÃ»r pour Ã©viter les problÃ¨mes de permissions
      - mkdir -p /tmp/ansible-safe
      - cp -r "${CI_PROJECT_DIR}"/* /tmp/ansible-safe/
      - cd /tmp/ansible-safe

      # ðŸ”¹ Copier et vÃ©rifier les variables de groupe pour Ansible
      - mkdir -p /tmp/ansible-safe/inventory/group_vars
      - cp -r "${CI_PROJECT_DIR}"/group_vars/* /tmp/ansible-safe/inventory/group_vars/ || true
      - echo "---- /tmp/ansible-safe/group_vars ----"
      - ls -l /tmp/ansible-safe/group_vars/ || true
      - echo "---- /tmp/ansible-safe/inventory/group_vars ----"
      - ls -l /tmp/ansible-safe/inventory/group_vars/ || true
      - echo "---- head of all.yml (if present) ----"
      - if [ -f /tmp/ansible-safe/inventory/group_vars/all.yml ]; then sed -n '1,120p' /tmp/ansible-safe/inventory/group_vars/all.yml; fi

      # ðŸ”¹ Config SSH
      - mkdir -p ~/.ssh
      - echo "$SSH_PRIVATE_KEY" > ~/.ssh/id_rsa
      - chmod 600 ~/.ssh/id_rsa
      - ssh-keyscan -H "$K8S_MASTER_IP" >> ~/.ssh/known_hosts || true
      - ssh-keyscan -H "$K8S_WORKER1_IP" >> ~/.ssh/known_hosts || true
      - ssh-keyscan -H "$K8S_WORKER2_IP" >> ~/.ssh/known_hosts || true

      # ðŸ”¹ Inventaire dynamique basÃ© sur variables GitLab CI/CD (expand variables)
      - |
        cat > inventory/inventory.yml << EOF
        all:
          children:
            masters:
              hosts:
                k8s-master-01:
                  ansible_host: $K8S_MASTER_IP
                  ansible_user: hocine
                  ansible_ssh_private_key_file: ~/.ssh/id_rsa
            workers:
              hosts:
                k8s-worker-01:
                  ansible_host: $K8S_WORKER1_IP
                  ansible_user: hocine
                  ansible_ssh_private_key_file: ~/.ssh/id_rsa
                k8s-worker-02:
                  ansible_host: $K8S_WORKER2_IP
                  ansible_user: hocine
                  ansible_ssh_private_key_file: ~/.ssh/id_rsa
          vars:
            ansible_python_interpreter: /usr/bin/python3
            ansible_ssh_common_args: '-o StrictHostKeyChecking=no'
        EOF

      # ðŸ”¹ DÃ©finir le fichier de config Ansible
      - export ANSIBLE_CONFIG="/tmp/ansible-safe/ansible.cfg"

      # ðŸ”¹ Diagnostic rapide
      - ansible --version
      - ansible-config dump | grep CONFIG_FILE || true

# ============================================================
# ðŸ§¹ Stage 1 : Nettoyage du cluster existant
# ============================================================

Cleanup_cluster:
  stage: Nettoyage
  <<: *ansible_job
  script:
    - echo "=== Nettoyage complet du cluster Kubernetes sur les VMs ==="
    - ansible-playbook playbooks/cleanup-playbook.yml
  allow_failure: true
  only:
    - infrastructure

# ============================================================
# âœ… Stage 2 : Validation (Syntaxe + Ping)
# ============================================================

Validation_syntaxe:
  stage: Validation
  <<: *ansible_job
  script:
    - echo "=== VÃ©rification de la syntaxe des playbooks ==="
    - ansible-playbook --syntax-check playbooks/site.yml
    - ansible-lint playbooks/site.yml || true
  only:
    - infrastructure

Validation_connexion:
  stage: Validation
  <<: *ansible_job
  script:
    - echo "=== VÃ©rification de la connectivitÃ© SSH aux VMs ==="
    - ansible all -m ping
  only:
    - infrastructure

# ============================================================
# ðŸ§° Stage 3 : PrÃ©paration de lâ€™environnement
# ============================================================

Preparation_environment:
  stage: PrÃ©paration
  <<: *ansible_job
  script:
    - echo "=== PrÃ©paration de lâ€™environnement des VMs ==="
    - ansible all -m setup
    - ansible all -m shell -a "uptime"
  artifacts:
    reports:
      junit: test-results.xml
    expire_in: 1 hour
  only:
    - infrastructure

Fix_k8s_repo:
  stage: PrÃ©paration
  <<: *ansible_job
  script:
    - echo "=== Fix Kubernetes apt repo (official apt.kubernetes.io) ==="
    - ansible-playbook playbooks/fix-k8s-repo.yml -v
  only:
    - infrastructure

# ============================================================
# ðŸš§ Stage 4 : DÃ©ploiement du cluster
# ============================================================

Deploy_common:
  stage: DÃ©ploiement-cluster
  <<: *ansible_job
  script:
    - echo "=== Configuration commune des nÅ“uds ==="
    - ansible-playbook playbooks/site.yml --tags common -v
  only:
    - infrastructure

Deploy_containerd:
  stage: DÃ©ploiement-cluster
  <<: *ansible_job
  script:
    - echo "=== Installation de Containerd ==="
    - ansible-playbook playbooks/site.yml --tags containerd -v
  dependencies:
    - Deploy_common
  only:
    - infrastructure

Deploy_kubernetes:
  stage: DÃ©ploiement-cluster
  <<: *ansible_job
  script:
    - echo "=== Installation de Kubernetes (binaries + kubeadm) ==="
    - ansible-playbook playbooks/site.yml --tags kubernetes -v
  dependencies:
    - Deploy_containerd
  only:
    - infrastructure

Deploy_master:
  stage: DÃ©ploiement-cluster
  <<: *ansible_job
  script:
    - echo "=== Configuration du nÅ“ud master ==="
    - ansible-playbook playbooks/site.yml --limit masters -v
  dependencies:
    - Deploy_kubernetes
  artifacts:
    paths:
      - kubeconfig/
      - dashboard-info/
      - logs/
    expire_in: 1 week
  only:
    - infrastructure

Deploy_workers:
  stage: DÃ©ploiement-cluster
  <<: *ansible_job
  script:
    - echo "=== Configuration des nÅ“uds workers ==="
    - ansible-playbook playbooks/site.yml --limit workers -v
  dependencies:
    - Deploy_master
  only:
    - infrastructure

# ============================================================
# ðŸ” Stage 5 : VÃ©rification du cluster
# ============================================================

Verification_cluster:
  stage: VÃ©rification
  <<: *ansible_job
  script:
    - echo "=== VÃ©rification du cluster Kubernetes ==="
    - mkdir -p ~/.kube
    - scp -o StrictHostKeyChecking=no hocine@$K8S_MASTER_IP:~/.kube/config ~/.kube/config || echo "Pas de kubeconfig trouvÃ©e"
    - kubectl get nodes -o wide
    - kubectl get pods -A
    - kubectl cluster-info
    - |
      echo "=== Test dâ€™un dÃ©ploiement simple (nginx-test) ==="
      kubectl create deployment nginx-test --image=nginx:latest || true
      kubectl expose deployment nginx-test --port=80 --type=NodePort || true
      sleep 30
      kubectl get pods -l app=nginx-test
      kubectl delete deployment nginx-test || true
      kubectl delete service nginx-test || true
  dependencies:
    - Deploy_workers
  artifacts:
    reports:
      junit: cluster-test-results.xml
    expire_in: 1 week
  only:
    - infrastructure

# ============================================================
# ðŸ“Š Stage 6 : DÃ©ploiement des applications (Dashboard + Monitoring)
# ============================================================

Deploy_dashboard:
  stage: DÃ©ploiement-app
  <<: *ansible_job
  script:
    - echo "=== DÃ©ploiement du Kubernetes Dashboard ==="
    - mkdir -p ~/.kube
    - scp -o StrictHostKeyChecking=no hocine@$K8S_MASTER_IP:~/.kube/config ~/.kube/config
    - kubectl apply -f configs/k8s-dashboard.yaml
    - sleep 60
    - kubectl get pods -n kubernetes-dashboard
    - kubectl get services -n kubernetes-dashboard
    - |
      echo "=== RÃ©cupÃ©ration du token dâ€™accÃ¨s Dashboard ==="
      TOKEN=$(kubectl -n kubernetes-dashboard get secret $(kubectl -n kubernetes-dashboard get sa admin-user -o jsonpath="{.secrets[0].name}") -o jsonpath="{.data.token}" | base64 -d)
      echo "Dashboard URL: https://$K8S_MASTER_IP:30443"
      echo "Token: $TOKEN"
      echo "Dashboard accessible Ã : https://$K8S_MASTER_IP:30443" > dashboard-access.txt
      echo "Token dâ€™accÃ¨s: $TOKEN" >> dashboard-access.txt
  dependencies:
    - Verification_cluster
  artifacts:
    paths:
      - dashboard-access.txt
    expire_in: 1 week
  only:
    - infrastructure

Deploy_monitoring:
  stage: DÃ©ploiement-app
  <<: *ansible_job
  script:
    - echo "=== DÃ©ploiement du stack de monitoring (Prometheus + Grafana) ==="
    - mkdir -p ~/.kube monitoring-config
    - scp -o StrictHostKeyChecking=no hocine@$K8S_MASTER_IP:~/.kube/config ~/.kube/config
    - kubectl apply -f configs/k8s-monitoring-stack.yaml
    - sleep 60
    - kubectl get pods -n kube-system | grep -E "prometheus|grafana|exporter" || true
    - ansible-playbook playbooks/prometheus-alerts.yml
  dependencies:
    - Deploy_dashboard
  artifacts:
    paths:
      - monitoring-config/
    expire_in: 1 week
  only:
    - infrastructure
